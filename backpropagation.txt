//Back-propagation
#include <iostream>
using namespace std;

#define MAX2(a,b) (a) > (b) ? (a) : (b)
class Neuron {
public: // attributes
	double w_; // weight of input
	double b_; //bias , 입력 여러개여도 bias 값은 하나
	double input_, output_; //save for back-prop
public: //
	Neuron() //default 생성자
		: w_(2.0), b_(1.0) {}
	Neuron(const double& w_input, const double& b_input)
		:w_(w_input), b_(b_input) {}
	// behaviors
	double getAct(const double& x) {
		//for linear or identity activation functions
		return x;
		//for ReLU activation fuctions
		//return MAX2(0.0, x);
	}
	void propBackward(const double& target) {

		const double alpha = 0.1; // learning rate
		const double grad = (output_ - target) * getActGrad(output_);
		w_ -= alpha * grad * input_; // last input_ came from d(wx+b)/dw=x;
		b_ -= alpha * grad * 1.0; // last 1.0 came from d(wx+b)/db=1

	}
	double feedForward(const double& _input) {
		//output y = f(\sigma),
		//  \ sigma = w_*input x + b
	    // for multiple inputs, \ sigma = w0_ * x0_ + w1_ * x1_ + .... + b
		input_ = _input;
		const double sigma = w_ * input_ + b_;
		output_= getAct(sigma);
		return output_;
	}
	double getActGrad(const double& x) {
		//for linear or identity activation functions
		return 1.0;;
		//ReLU if(x>0.0) return 1.0; else 0.0;
		//return MAX2(0.0, x);
		//return 1.0;
	}
	void feedForwardAndPrint(const double& input) {
		cout << input << ' ' << feedForward(input) << '\n';
	}
};

int main() {
	Neuron my_neuron(2.0, 1.0); //initialize my_neuron
	for (int i = 0; i < 100; i++) {
		cout << "training " << i << '\n';
		my_neuron.feedForwardAndPrint(1.0);
		my_neuron.propBackward(4.0);
		cout << "w = " << my_neuron.w_ << " b = " << my_neuron.b_ << '\n';
		
	}
}